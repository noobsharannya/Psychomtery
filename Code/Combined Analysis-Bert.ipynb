{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e10253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import beta\n",
    "import pickle\n",
    "from scipy.stats import pearsonr\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,mean_squared_error\n",
    "from itertools import combinations\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5633c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('CSV/bert_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638ed562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml(X,labels):\n",
    "    split_index = 17838\n",
    "    labels=np.array(labels)\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = labels[:split_index], labels[split_index:]\n",
    "    logistic_regression_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    logistic_regression_classifier.fit(X_train, y_train)\n",
    "    y_pred = logistic_regression_classifier.predict(X_test)\n",
    "    probabilities = logistic_regression_classifier.predict_proba(X_test)\n",
    "    confidences = np.max(probabilities, axis=1)\n",
    "    logistic_confidence=np.mean(confidences)\n",
    "    acc_logistic=accuracy_score(y_test, y_pred)\n",
    "    svm_classifier = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    confidence_scores = svm_classifier.decision_function(X_test)\n",
    "    svm_confidence=np.mean(np.abs(confidence_scores))\n",
    "    acc_svm=accuracy_score(y_test, y_pred)\n",
    "    h1=len(X_train[0])\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(2*h1,6), max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    probabilities = mlp.predict_proba(X_test)\n",
    "    confidences = np.max(probabilities, axis=1)\n",
    "    mlp_confidence=np.mean(confidences)\n",
    "    acc_mlp=accuracy_score(y_test, y_pred)\n",
    "    random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    random_forest_classifier.fit(X_train, y_train)\n",
    "    y_pred = random_forest_classifier.predict(X_test)\n",
    "    probabilities = random_forest_classifier.predict_proba(X_test)\n",
    "    confidences = np.max(probabilities, axis=1)\n",
    "    rf_confidence=np.mean(confidences)\n",
    "    acc_rf=accuracy_score(y_test, y_pred)\n",
    "    data=[acc_logistic,acc_svm,acc_mlp,acc_rf]\n",
    "    print(data)\n",
    "    confidence=[logistic_confidence,svm_confidence,mlp_confidence,rf_confidence]\n",
    "    return confidence,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb5a3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_order():\n",
    "    truncate_index = 17838\n",
    "    truncated_df = df.iloc[:truncate_index]\n",
    "    indices=[i for i in range(124)]\n",
    "    f=len(indices)\n",
    "    y= list(truncated_df['Final Label'])\n",
    "    values=list(truncated_df['Composite'])\n",
    "    X=[]\n",
    "    for i in indices:\n",
    "        X.append(list(truncated_df[f'feature{i}']))\n",
    "    l=len(X[0])\n",
    "    corr_matrix=np.zeros((f,3))\n",
    "    weight_matrix=np.zeros((f,3))\n",
    "    for i in range(3):\n",
    "        c=[values[j] for j in range(l) if y[j]==i]\n",
    "        if(len(c)>0):\n",
    "            for j in range(f):\n",
    "                x_temp=[X[j][k] for k in range(l) if y[k]==i]\n",
    "                corr, _ = pearsonr(x_temp, c)\n",
    "                corr_matrix[j][i]=abs(corr)\n",
    "            x=list(corr_matrix[:,i])\n",
    "            rank_indices=np.argsort(x)\n",
    "            rank=1\n",
    "            for idx in rank_indices:\n",
    "                weight_matrix[idx][i]=rank\n",
    "                rank=rank+1\n",
    "    correlation=np.zeros((f,f))\n",
    "    for i in range(f):\n",
    "        for j in range(f):\n",
    "            x1=X[i][:]\n",
    "            x2=X[j][:]\n",
    "            corr, _ = pearsonr(x1,x2)\n",
    "            correlation[i][j]=corr\n",
    "    scores=[]\n",
    "    for i in range(f):\n",
    "        weight_sum=0\n",
    "        corr_sum=0\n",
    "        for j in range(3):\n",
    "            corr_sum=corr_sum+(corr_matrix[i][j]*weight_matrix[i][j])\n",
    "            weight_sum=weight_sum+weight_matrix[i][j]\n",
    "        scores.append((corr_sum*f)/(weight_sum*np.sum(correlation[i])))\n",
    "    rank_features=np.argsort(scores)\n",
    "    l=len(scores)-1\n",
    "    correct_order=[]\n",
    "    for i in range(f):\n",
    "        correct_order.append(indices[rank_features[l]])\n",
    "        l=l-1\n",
    "    return correct_order\n",
    "def correlation_algo(correct_order):\n",
    "    labels=list(df['Final Label'])\n",
    "    data=[]\n",
    "    confidence=[]\n",
    "    for i in range(1,124):\n",
    "        refined_set=correct_order[:i]\n",
    "        discarded_set=correct_order[i:]\n",
    "        print(f'Number of features: {i}')\n",
    "        columns=[f'feature{k}' for k in refined_set]\n",
    "        X1=df[columns].values\n",
    "        con,info=ml(X1,labels)\n",
    "        print(max(info))\n",
    "        data.append(info)\n",
    "        confidence.append(con)\n",
    "    return confidence,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda44d9b-f723-467c-96b8-feb477ffb01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105, 102, 51, 91, 40, 111, 54, 117, 122, 14, 72, 25, 1, 45, 21, 5, 29, 9, 87, 17, 2, 101, 4, 95, 19, 15, 13, 77, 112, 3, 36, 81, 79, 16, 34, 24, 20, 23, 0, 53, 37, 113, 106, 22, 55, 65, 31, 12, 33, 83, 66, 46, 50, 97, 108, 32, 63, 27, 58, 73, 35, 38, 30, 56, 68, 42, 8, 70, 62, 49, 26, 52, 114, 67, 43, 11, 18, 7, 80, 64, 75, 28, 47, 10, 121, 71, 48, 61, 116, 39, 59, 6, 104, 44, 94, 110, 78, 107, 69, 60, 57, 86, 74, 90, 96, 120, 93, 99, 76, 109, 115, 92, 89, 98, 41, 88, 119, 100, 82, 84, 123, 118, 85, 103]\n",
      "Number of features: 1\n",
      "[0.6847241336169148, 0.6847241336169148, 0.6847241336169148, 0.6847241336169148]\n",
      "0.6847241336169148\n",
      "Number of features: 2\n",
      "[0.6847241336169148, 0.6847241336169148, 0.6847241336169148, 0.6847241336169148]\n",
      "0.6847241336169148\n",
      "Number of features: 3\n",
      "[0.6847241336169148, 0.6847241336169148, 0.6847241336169148, 0.6847241336169148]\n",
      "0.6847241336169148\n",
      "Number of features: 4\n",
      "[0.6847241336169148, 0.6847241336169148, 0.6847241336169148, 0.6170399099211811]\n",
      "0.6847241336169148\n",
      "Number of features: 5\n",
      "[0.6847241336169148, 0.6847241336169148, 0.6847241336169148, 0.6281746528212185]\n",
      "0.6847241336169148\n",
      "Number of features: 6\n",
      "[0.6847241336169148, 0.6847241336169148, 0.6830977104966846, 0.6338045790066308]\n",
      "0.6847241336169148\n",
      "Number of features: 7\n",
      "[0.6847241336169148, 0.6847241336169148, 0.6871012135618666, 0.6501939196797197]\n",
      "0.6871012135618666\n",
      "Number of features: 8\n",
      "[0.6847241336169148, 0.6847241336169148, 0.6642061804078568, 0.6574502689853622]\n",
      "0.6847241336169148\n",
      "Number of features: 9\n",
      "[0.6847241336169148, 0.6833479294382585, 0.6625797572876266, 0.6703365444764169]\n",
      "0.6847241336169148\n",
      "Number of features: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBSCHOLOR-2020-18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6847241336169148, 0.6842236957337671, 0.6550731890404103, 0.6983610659326911]\n",
      "0.6983610659326911\n",
      "Number of features: 11\n",
      "[0.6847241336169148, 0.6839734767921931, 0.6540723132741149, 0.6956086575753785]\n",
      "0.6956086575753785\n",
      "Number of features: 12\n",
      "[0.6847241336169148, 0.6779682221944201, 0.6506943575628675, 0.7001125985237082]\n",
      "0.7001125985237082\n",
      "Number of features: 13\n",
      "[0.6847241336169148, 0.676216689603403, 0.653696984861754, 0.6981108469911173]\n",
      "0.6981108469911173\n",
      "Number of features: 14\n",
      "[0.6847241336169148, 0.6769673464281246, 0.5996496934817965, 0.6988615038158389]\n",
      "0.6988615038158389\n",
      "Number of features: 15\n",
      "[0.6847241336169148, 0.6744651570123859, 0.62116852245715, 0.6894782935068184]\n",
      "0.6894782935068184\n",
      "Number of features: 16\n",
      "[0.6847241336169148, 0.6665832603528087, 0.5922682347053672, 0.6958588765169523]\n",
      "0.6958588765169523\n",
      "Number of features: 17\n",
      "[0.6847241336169148, 0.6662079319404479, 0.625297134993119, 0.700613036406856]\n",
      "0.700613036406856\n",
      "Number of features: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBSCHOLOR-2020-18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6847241336169148, 0.6682096834730389, 0.5816339296884774, 0.7011134742900037]\n",
      "0.7011134742900037\n",
      "Number of features: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBSCHOLOR-2020-18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6847241336169148, 0.6585762542224446, 0.5616164143625673, 0.6974852996371825]\n",
      "0.6974852996371825\n",
      "Number of features: 20\n",
      "[0.6847241336169148, 0.6695858876516952, 0.5942699862379582, 0.6986112848742649]\n",
      "0.6986112848742649\n",
      "Number of features: 21\n",
      "[0.6847241336169148, 0.6633304141123483, 0.5491054672838734, 0.7003628174652822]\n",
      "0.7003628174652822\n",
      "Number of features: 22\n",
      "[0.6847241336169148, 0.663955961466283, 0.5917677968222195, 0.7001125985237082]\n",
      "0.7001125985237082\n",
      "Number of features: 23\n",
      "[0.6847241336169148, 0.6674590266483172, 0.5726260477918178, 0.7044914300012511]\n",
      "0.7044914300012511\n",
      "Number of features: 24\n",
      "[0.6837232578506193, 0.6738396096584511, 0.5850118853997247, 0.7023645689978731]\n",
      "0.7023645689978731\n",
      "Number of features: 25\n",
      "[0.6837232578506193, 0.6782184411359939, 0.5897660452896284, 0.7023645689978731]\n",
      "0.7023645689978731\n",
      "Number of features: 26\n",
      "[0.6837232578506193, 0.6874765419742275, 0.5913924684098586, 0.7022394595270862]\n",
      "0.7022394595270862\n",
      "Number of features: 27\n",
      "[0.6829726010258976, 0.6963593144001001, 0.6022769923683223, 0.7048667584136119]\n",
      "0.7048667584136119\n",
      "Number of features: 28\n",
      "[0.6829726010258976, 0.6993619416989866, 0.6047791817840611, 0.7082447141248592]\n",
      "0.7082447141248592\n",
      "Number of features: 29\n",
      "[0.6829726010258976, 0.6997372701113475, 0.6257975728762667, 0.7059927436506943]\n",
      "0.7059927436506943\n",
      "Number of features: 30\n",
      "[0.6854747904416364, 0.7038658826473164, 0.6041536344301264, 0.7211309896159139]\n",
      "0.7211309896159139\n",
      "Number of features: 31\n",
      "[0.6873514325034406, 0.7172525960215188, 0.6132866257975729, 0.7132490929563368]\n",
      "0.7172525960215188\n",
      "Number of features: 32\n",
      "[0.6850994620292756, 0.7081196046540723, 0.6165394720380333, 0.7175028149630928]\n",
      "0.7175028149630928\n",
      "Number of features: 33\n",
      "[0.6858501188539973, 0.706743400475416, 0.6214187413987239, 0.7076191667709245]\n",
      "0.7076191667709245\n",
      "Number of features: 34\n",
      "[0.6845990241461278, 0.7002377079944951, 0.6303015138245965, 0.7161266107844364]\n",
      "0.7161266107844364\n",
      "Number of features: 35\n",
      "[0.6859752283247842, 0.7041161015888903, 0.6485674965594895, 0.7137495308394846]\n",
      "0.7137495308394846\n",
      "Number of features: 36\n",
      "[0.686225447266358, 0.7004879269360691, 0.6214187413987239, 0.7195045664956837]\n",
      "0.7195045664956837\n",
      "Number of features: 37\n",
      "[0.686225447266358, 0.7043663205304641, 0.6553234079819842, 0.7181283623170274]\n",
      "0.7181283623170274\n",
      "Number of features: 38\n",
      "[0.6868509946202928, 0.7116226698361066, 0.6399349430751908, 0.7387714249968723]\n",
      "0.7387714249968723\n",
      "Number of features: 39\n",
      "[0.6876016514450144, 0.7123733266608282, 0.6181658951582635, 0.7240085074440135]\n",
      "0.7240085074440135\n",
      "Number of features: 40\n",
      "[0.6911047166270486, 0.7108720130113849, 0.6489428249718504, 0.723007631677718]\n",
      "0.723007631677718\n",
      "Number of features: 41\n",
      "[0.6894782935068184, 0.7106217940698111, 0.6428124609032904, 0.7263855873889653]\n",
      "0.7263855873889653\n",
      "Number of features: 42\n",
      "[0.6901038408607532, 0.7111222319529589, 0.6484423870887026, 0.7362692355811334]\n",
      "0.7362692355811334\n",
      "Number of features: 43\n",
      "[0.6897285124483924, 0.7037407731765295, 0.6514450143875892, 0.7313899662204428]\n",
      "0.7313899662204428\n",
      "Number of features: 44\n",
      "[0.6901038408607532, 0.7016139121731515, 0.6388089578381083, 0.7345177029901163]\n",
      "0.7345177029901163\n",
      "Number of features: 45\n",
      "[0.690354059802327, 0.6998623795821344, 0.6381834104841736, 0.7310146378080821]\n",
      "0.7310146378080821\n",
      "Number of features: 46\n",
      "[0.6933566871012136, 0.7053671962967597, 0.6539472038033279, 0.7363943450519205]\n",
      "0.7363943450519205\n",
      "Number of features: 47\n",
      "[0.6916051545101964, 0.7091204804203678, 0.6403102714875516, 0.7333917177530339]\n",
      "0.7333917177530339\n",
      "Number of features: 48\n",
      "[0.6919804829225572, 0.7024896784686601, 0.6326785937695484, 0.7357687976979858]\n",
      "0.7357687976979858\n",
      "Number of features: 49\n",
      "[0.6917302639809834, 0.7064931815338421, 0.6413111472538471, 0.7375203302890029]\n",
      "0.7375203302890029\n",
      "Number of features: 50\n",
      "[0.6923558113349181, 0.6989866132866258, 0.6426873514325034, 0.7337670461653947]\n",
      "0.7337670461653947\n",
      "Number of features: 51\n",
      "[0.6914800450394095, 0.6894782935068184, 0.6625797572876266, 0.7227574127361441]\n",
      "0.7227574127361441\n",
      "Number of features: 52\n",
      "[0.6904791692731139, 0.6963593144001001, 0.6290504191167271, 0.7376454397597898]\n",
      "0.7376454397597898\n",
      "Number of features: 53\n",
      "[0.6931064681596397, 0.6917302639809834, 0.6704616539472038, 0.737270111347429]\n",
      "0.737270111347429\n",
      "Number of features: 54\n",
      "[0.6947328912798699, 0.7001125985237082, 0.670086325534843, 0.7360190166395596]\n",
      "0.7360190166395596\n",
      "Number of features: 55\n",
      "[0.6916051545101964, 0.7082447141248592, 0.6556987363943451, 0.7320155135743776]\n",
      "0.7320155135743776\n",
      "Number of features: 56\n",
      "[0.6934817965720005, 0.7167521581383711, 0.6615788815213312, 0.7320155135743776]\n",
      "0.7320155135743776\n",
      "Number of features: 57\n",
      "[0.6971099712248218, 0.722882522206931, 0.6638308519954961, 0.7321406230451645]\n",
      "0.7321406230451645\n",
      "Number of features: 58\n",
      "[0.6954835481045916, 0.7166270486675841, 0.6459401976729638, 0.7415238333541849]\n",
      "0.7415238333541849\n",
      "Number of features: 59\n",
      "[0.7056174152383335, 0.7180032528462404, 0.6627048667584136, 0.7232578506192919]\n",
      "0.7232578506192919\n",
      "Number of features: 60\n",
      "[0.7029901163518079, 0.7173777054923057, 0.6583260352808707, 0.737395220818216]\n",
      "0.737395220818216\n",
      "Number of features: 61\n",
      "[0.7003628174652822, 0.7178781433754535, 0.6403102714875516, 0.7445264606530715]\n",
      "0.7445264606530715\n",
      "Number of features: 62\n",
      "[0.7018641311147253, 0.7203803327911923, 0.6568247216314275, 0.7451520080070061]\n",
      "0.7451520080070061\n",
      "Number of features: 63\n",
      "[0.7024896784686601, 0.7157512823720755, 0.6893531840360315, 0.7401476291755286]\n",
      "0.7401476291755286\n",
      "Number of features: 64\n",
      "[0.7023645689978731, 0.7133742024271237, 0.6473164018516202, 0.7429000375328413]\n",
      "0.7429000375328413\n",
      "Number of features: 65\n",
      "[0.7024896784686601, 0.721005880145127, 0.6785937695483548, 0.7375203302890029]\n",
      "0.7375203302890029\n",
      "Number of features: 66\n",
      "[0.7036156637057426, 0.7235080695608658, 0.6673339171775303, 0.743400475415989]\n",
      "0.743400475415989\n",
      "Number of features: 67\n",
      "[0.7019892405855124, 0.7242587263855874, 0.6775928937820593, 0.7356436882271988]\n",
      "0.7356436882271988\n",
      "Number of features: 68\n",
      "[0.6976104091079695, 0.725009383210309, 0.6602026773426748, 0.7346428124609032]\n",
      "0.7346428124609032\n",
      "Number of features: 69\n",
      "[0.6976104091079695, 0.725009383210309, 0.6902289503315401, 0.7397723007631678]\n",
      "0.7397723007631678\n",
      "Number of features: 70\n",
      "[0.7011134742900037, 0.7285124483923433, 0.6784686600775679, 0.7356436882271988]\n",
      "0.7356436882271988\n",
      "Number of features: 71\n",
      "[0.696609533341674, 0.7316401851620168, 0.6993619416989866, 0.7343925935193294]\n",
      "0.7343925935193294\n",
      "Number of features: 72\n",
      "[0.6977355185787564, 0.7316401851620168, 0.6967346428124609, 0.741148504941824]\n",
      "0.741148504941824\n",
      "Number of features: 73\n",
      "[0.7013636932315777, 0.7305141999249343, 0.6572000500437883, 0.743400475415989]\n",
      "0.743400475415989\n",
      "Number of features: 74\n",
      "[0.7023645689978731, 0.7260102589766045, 0.6625797572876266, 0.7465282121856625]\n",
      "0.7465282121856625\n",
      "Number of features: 75\n",
      "[0.6974852996371825, 0.7177530339046666, 0.6629550856999875, 0.743275365945202]\n",
      "0.743275365945202\n",
      "Number of features: 76\n",
      "[0.698736394345052, 0.7200050043788315, 0.6848492430877018, 0.7439009132991368]\n",
      "0.7439009132991368\n",
      "Number of features: 77\n",
      "[0.6977355185787564, 0.7212560990867009, 0.6793444263730765, 0.7415238333541849]\n",
      "0.7415238333541849\n",
      "Number of features: 78\n",
      "[0.6976104091079695, 0.720755661203553, 0.6590766921055924, 0.7466533216564494]\n",
      "0.7466533216564494\n",
      "Number of features: 79\n",
      "[0.6983610659326911, 0.716877267609158, 0.6858501188539973, 0.7440260227699237]\n",
      "0.7440260227699237\n",
      "Number of features: 80\n",
      "[0.696609533341674, 0.7157512823720755, 0.6556987363943451, 0.753659452020518]\n",
      "0.753659452020518\n",
      "Number of features: 81\n",
      "[0.6926060302764919, 0.7153759539597148, 0.6682096834730389, 0.7456524458901539]\n",
      "0.7456524458901539\n",
      "Number of features: 82\n",
      "[0.690604278743901, 0.7196296759664706, 0.6773426748404854, 0.7447766795946453]\n",
      "0.7447766795946453\n",
      "Number of features: 83\n",
      "[0.6886025272113099, 0.7257600400350307, 0.6748404854247466, 0.743400475415989]\n",
      "0.743400475415989\n",
      "Number of features: 84\n",
      "[0.6898536219191793, 0.7208807706743401, 0.6887276366820968, 0.7482797447766796]\n",
      "0.7482797447766796\n",
      "Number of features: 85\n",
      "[0.6914800450394095, 0.7223820843237833, 0.6929813586888528, 0.7430251470036282]\n",
      "0.7430251470036282\n",
      "Number of features: 86\n",
      "[0.6901038408607532, 0.7177530339046666, 0.6904791692731139, 0.749530839484549]\n",
      "0.749530839484549\n",
      "Number of features: 87\n",
      "[0.6902289503315401, 0.7180032528462404, 0.6738396096584511, 0.7425247091204804]\n",
      "0.7425247091204804\n",
      "Number of features: 88\n",
      "[0.6889778556236708, 0.7190041286125359, 0.6848492430877018, 0.7480295258351057]\n",
      "0.7480295258351057\n",
      "Number of features: 89\n",
      "[0.6894782935068184, 0.7217565369698486, 0.6594520205179533, 0.7470286500688103]\n",
      "0.7470286500688103\n",
      "Number of features: 90\n",
      "[0.690604278743901, 0.7221318653822094, 0.6896034029776054, 0.743400475415989]\n",
      "0.743400475415989\n",
      "Number of features: 91\n",
      "[0.6916051545101964, 0.7213812085574878, 0.6749655948955335, 0.7479044163643188]\n",
      "0.7479044163643188\n",
      "Number of features: 92\n",
      "[0.6961090954585262, 0.7162517202552233, 0.6775928937820593, 0.7487801826598274]\n",
      "0.7487801826598274\n",
      "Number of features: 93\n",
      "[0.6952333291630176, 0.7175028149630928, 0.6578255973977231, 0.7441511322407106]\n",
      "0.7441511322407106\n",
      "Number of features: 94\n",
      "[0.6952333291630176, 0.7171274865507319, 0.6981108469911173, 0.7471537595395972]\n",
      "0.7471537595395972\n",
      "Number of features: 95\n",
      "[0.6949831102214438, 0.7139997497810584, 0.6967346428124609, 0.7500312773676967]\n",
      "0.7500312773676967\n",
      "Number of features: 96\n",
      "[0.690604278743901, 0.7129988740147629, 0.6887276366820968, 0.7511572626047792]\n",
      "0.7511572626047792\n",
      "Number of features: 97\n",
      "[0.6918553734517703, 0.7200050043788315, 0.6777180032528463, 0.7489052921306143]\n",
      "0.7489052921306143\n",
      "Number of features: 98\n",
      "[0.6912298260978356, 0.7208807706743401, 0.6914800450394095, 0.743400475415989]\n",
      "0.743400475415989\n",
      "Number of features: 99\n",
      "[0.6922307018641312, 0.7153759539597148, 0.6932315776304266, 0.7357687976979858]\n",
      "0.7357687976979858\n",
      "Number of features: 100\n",
      "[0.6927311397472788, 0.722882522206931, 0.6817215063180283, 0.7437758038283498]\n",
      "0.7437758038283498\n",
      "Number of features: 101\n",
      "[0.6919804829225572, 0.7231327411485049, 0.6588264731640185, 0.747403978481171]\n",
      "0.747403978481171\n",
      "Number of features: 102\n",
      "[0.6914800450394095, 0.7162517202552233, 0.7046165394720381, 0.7437758038283498]\n",
      "0.7437758038283498\n",
      "Number of features: 103\n",
      "[0.6894782935068184, 0.710997122482172, 0.6927311397472788, 0.7397723007631678]\n",
      "0.7397723007631678\n",
      "Number of features: 104\n",
      "[0.6899787313899662, 0.7158763918428626, 0.7053671962967597, 0.735143250344051]\n",
      "0.735143250344051\n",
      "Number of features: 105\n",
      "[0.6857250093832103, 0.7165019391967972, 0.6881020893281622, 0.7447766795946453]\n",
      "0.7447766795946453\n",
      "Number of features: 106\n",
      "[0.6789690979607156, 0.7157512823720755, 0.6441886650819467, 0.7451520080070061]\n",
      "0.7451520080070061\n",
      "Number of features: 107\n",
      "[0.6809708494933067, 0.7193794570248968, 0.6703365444764169, 0.7367696734642812]\n",
      "0.7367696734642812\n",
      "Number of features: 108\n",
      "[0.6792193169022895, 0.717002377079945, 0.6769673464281246, 0.7426498185912673]\n",
      "0.7426498185912673\n",
      "Number of features: 109\n",
      "[0.6778431127236332, 0.7167521581383711, 0.6937320155135743, 0.7480295258351057]\n",
      "0.7480295258351057\n",
      "Number of features: 110\n",
      "[0.676216689603403, 0.7157512823720755, 0.6818466157888152, 0.7484048542474665]\n",
      "0.7484048542474665\n",
      "Number of features: 111\n",
      "[0.6712123107719254, 0.71475040660578, 0.694607781809083, 0.7342674840485425]\n",
      "0.7342674840485425\n",
      "Number of features: 112\n",
      "[0.6683347929438258, 0.7171274865507319, 0.6803453021393719, 0.733016389340673]\n",
      "0.733016389340673\n",
      "Number of features: 113\n",
      "[0.6749655948955335, 0.7186288002001752, 0.6889778556236708, 0.7391467534092331]\n",
      "0.7391467534092331\n",
      "Number of features: 114\n",
      "[0.6744651570123859, 0.7176279244338797, 0.6772175653696985, 0.7410233954710371]\n",
      "0.7410233954710371\n",
      "Number of features: 115\n",
      "[0.6718378581258602, 0.7033654447641686, 0.6698361065932691, 0.7410233954710371]\n",
      "0.7410233954710371\n",
      "Number of features: 116\n",
      "[0.6732140623045164, 0.7046165394720381, 0.6554485174527712, 0.7472788690103841]\n",
      "0.7472788690103841\n",
      "Number of features: 117\n",
      "[0.6735893907168773, 0.7137495308394846, 0.6820968347303891, 0.7337670461653947]\n",
      "0.7337670461653947\n",
      "Number of features: 118\n",
      "[0.6749655948955335, 0.7145001876642062, 0.6937320155135743, 0.7507819341924183]\n",
      "0.7507819341924183\n",
      "Number of features: 119\n",
      "[0.6743400475415989, 0.7161266107844364, 0.6630801951707744, 0.7406480670586764]\n",
      "0.7406480670586764\n",
      "Number of features: 120\n",
      "[0.673964719129238, 0.7099962467158764, 0.6892280745652446, 0.741148504941824]\n",
      "0.741148504941824\n",
      "Number of features: 121\n",
      "[0.6745902664831728, 0.7123733266608282, 0.7014888027023646, 0.7469035405980232]\n",
      "0.7469035405980232\n",
      "Number of features: 122\n",
      "[0.6754660327786813, 0.7111222319529589, 0.6569498311022144, 0.7365194545227074]\n",
      "0.7365194545227074\n",
      "Number of features: 123\n",
      "[0.6745902664831728, 0.7084949330664331, 0.6833479294382585, 0.7326410609283123]\n",
      "0.7326410609283123\n",
      "[0.6749655948955335, 0.7094958088327287, 0.6889778556236708, 0.7430251470036282]\n",
      "Accuracy on augmented data\n",
      "+------------+----------+----------+-----------------+\n",
      "|   Logistic |      SVM |      MLP |   Random Forest |\n",
      "+============+==========+==========+=================+\n",
      "|   0.684724 | 0.684724 | 0.684724 |        0.684724 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.684724 | 0.684724 |        0.684724 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.684724 | 0.684724 |        0.684724 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.684724 | 0.684724 |        0.61704  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.684724 | 0.684724 |        0.628175 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.684724 | 0.683098 |        0.633805 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.684724 | 0.687101 |        0.650194 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.684724 | 0.664206 |        0.65745  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.683348 | 0.66258  |        0.670337 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.684224 | 0.655073 |        0.698361 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.683973 | 0.654072 |        0.695609 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.677968 | 0.650694 |        0.700113 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.676217 | 0.653697 |        0.698111 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.676967 | 0.59965  |        0.698862 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.674465 | 0.621169 |        0.689478 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.666583 | 0.592268 |        0.695859 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.666208 | 0.625297 |        0.700613 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.66821  | 0.581634 |        0.701113 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.658576 | 0.561616 |        0.697485 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.669586 | 0.59427  |        0.698611 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.66333  | 0.549105 |        0.700363 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.663956 | 0.591768 |        0.700113 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684724 | 0.667459 | 0.572626 |        0.704491 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.683723 | 0.67384  | 0.585012 |        0.702365 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.683723 | 0.678218 | 0.589766 |        0.702365 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.683723 | 0.687477 | 0.591392 |        0.702239 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.682973 | 0.696359 | 0.602277 |        0.704867 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.682973 | 0.699362 | 0.604779 |        0.708245 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.682973 | 0.699737 | 0.625798 |        0.705993 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.685475 | 0.703866 | 0.604154 |        0.721131 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.687351 | 0.717253 | 0.613287 |        0.713249 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.685099 | 0.70812  | 0.616539 |        0.717503 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.68585  | 0.706743 | 0.621419 |        0.707619 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.684599 | 0.700238 | 0.630302 |        0.716127 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.685975 | 0.704116 | 0.648567 |        0.71375  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.686225 | 0.700488 | 0.621419 |        0.719505 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.686225 | 0.704366 | 0.655323 |        0.718128 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.686851 | 0.711623 | 0.639935 |        0.738771 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.687602 | 0.712373 | 0.618166 |        0.724009 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.691105 | 0.710872 | 0.648943 |        0.723008 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.689478 | 0.710622 | 0.642812 |        0.726386 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.690104 | 0.711122 | 0.648442 |        0.736269 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.689729 | 0.703741 | 0.651445 |        0.73139  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.690104 | 0.701614 | 0.638809 |        0.734518 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.690354 | 0.699862 | 0.638183 |        0.731015 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.693357 | 0.705367 | 0.653947 |        0.736394 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.691605 | 0.70912  | 0.64031  |        0.733392 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69198  | 0.70249  | 0.632679 |        0.735769 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69173  | 0.706493 | 0.641311 |        0.73752  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.692356 | 0.698987 | 0.642687 |        0.733767 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69148  | 0.689478 | 0.66258  |        0.722757 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.690479 | 0.696359 | 0.62905  |        0.737645 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.693106 | 0.69173  | 0.670462 |        0.73727  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.694733 | 0.700113 | 0.670086 |        0.736019 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.691605 | 0.708245 | 0.655699 |        0.732016 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.693482 | 0.716752 | 0.661579 |        0.732016 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69711  | 0.722883 | 0.663831 |        0.732141 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.695484 | 0.716627 | 0.64594  |        0.741524 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.705617 | 0.718003 | 0.662705 |        0.723258 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.70299  | 0.717378 | 0.658326 |        0.737395 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.700363 | 0.717878 | 0.64031  |        0.744526 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.701864 | 0.72038  | 0.656825 |        0.745152 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.70249  | 0.715751 | 0.689353 |        0.740148 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.702365 | 0.713374 | 0.647316 |        0.7429   |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.70249  | 0.721006 | 0.678594 |        0.73752  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.703616 | 0.723508 | 0.667334 |        0.7434   |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.701989 | 0.724259 | 0.677593 |        0.735644 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69761  | 0.725009 | 0.660203 |        0.734643 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69761  | 0.725009 | 0.690229 |        0.739772 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.701113 | 0.728512 | 0.678469 |        0.735644 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69661  | 0.73164  | 0.699362 |        0.734393 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.697736 | 0.73164  | 0.696735 |        0.741149 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.701364 | 0.730514 | 0.6572   |        0.7434   |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.702365 | 0.72601  | 0.66258  |        0.746528 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.697485 | 0.717753 | 0.662955 |        0.743275 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.698736 | 0.720005 | 0.684849 |        0.743901 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.697736 | 0.721256 | 0.679344 |        0.741524 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69761  | 0.720756 | 0.659077 |        0.746653 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.698361 | 0.716877 | 0.68585  |        0.744026 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69661  | 0.715751 | 0.655699 |        0.753659 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.692606 | 0.715376 | 0.66821  |        0.745652 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.690604 | 0.71963  | 0.677343 |        0.744777 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.688603 | 0.72576  | 0.67484  |        0.7434   |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.689854 | 0.720881 | 0.688728 |        0.74828  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69148  | 0.722382 | 0.692981 |        0.743025 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.690104 | 0.717753 | 0.690479 |        0.749531 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.690229 | 0.718003 | 0.67384  |        0.742525 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.688978 | 0.719004 | 0.684849 |        0.74803  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.689478 | 0.721757 | 0.659452 |        0.747029 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.690604 | 0.722132 | 0.689603 |        0.7434   |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.691605 | 0.721381 | 0.674966 |        0.747904 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.696109 | 0.716252 | 0.677593 |        0.74878  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.695233 | 0.717503 | 0.657826 |        0.744151 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.695233 | 0.717127 | 0.698111 |        0.747154 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.694983 | 0.714    | 0.696735 |        0.750031 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.690604 | 0.712999 | 0.688728 |        0.751157 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.691855 | 0.720005 | 0.677718 |        0.748905 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69123  | 0.720881 | 0.69148  |        0.7434   |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.692231 | 0.715376 | 0.693232 |        0.735769 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.692731 | 0.722883 | 0.681722 |        0.743776 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69198  | 0.723133 | 0.658826 |        0.747404 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.69148  | 0.716252 | 0.704617 |        0.743776 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.689478 | 0.710997 | 0.692731 |        0.739772 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.689979 | 0.715876 | 0.705367 |        0.735143 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.685725 | 0.716502 | 0.688102 |        0.744777 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.678969 | 0.715751 | 0.644189 |        0.745152 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.680971 | 0.719379 | 0.670337 |        0.73677  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.679219 | 0.717002 | 0.676967 |        0.74265  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.677843 | 0.716752 | 0.693732 |        0.74803  |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.676217 | 0.715751 | 0.681847 |        0.748405 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.671212 | 0.71475  | 0.694608 |        0.734267 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.668335 | 0.717127 | 0.680345 |        0.733016 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.674966 | 0.718629 | 0.688978 |        0.739147 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.674465 | 0.717628 | 0.677218 |        0.741023 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.671838 | 0.703365 | 0.669836 |        0.741023 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.673214 | 0.704617 | 0.655449 |        0.747279 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.673589 | 0.71375  | 0.682097 |        0.733767 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.674966 | 0.7145   | 0.693732 |        0.750782 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.67434  | 0.716127 | 0.66308  |        0.740648 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.673965 | 0.709996 | 0.689228 |        0.741149 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.67459  | 0.712373 | 0.701489 |        0.746904 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.675466 | 0.711122 | 0.65695  |        0.736519 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.67459  | 0.708495 | 0.683348 |        0.732641 |\n",
      "+------------+----------+----------+-----------------+\n",
      "|   0.674966 | 0.709496 | 0.688978 |        0.743025 |\n",
      "+------------+----------+----------+-----------------+\n",
      "Confidence on augmented data\n",
      "+------------+---------+----------+-----------------+\n",
      "|   Logistic |     SVM |      MLP |   Random Forest |\n",
      "+============+=========+==========+=================+\n",
      "|   0.769712 | 1.1427  | 0.769157 |        0.771085 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.769504 | 1.1314  | 0.770018 |        0.769938 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772207 | 1.13211 | 0.767146 |        0.773912 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.782892 | 1.13201 | 0.779812 |        0.810752 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.782452 | 1.13589 | 0.77665  |        0.825744 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.781801 | 1.14346 | 0.777482 |        0.802481 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.782808 | 1.14633 | 0.773216 |        0.771596 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.782719 | 1.14432 | 0.765988 |        0.764566 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.784321 | 1.14161 | 0.784412 |        0.750672 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.784255 | 1.14271 | 0.77772  |        0.738495 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.785037 | 1.14704 | 0.8181   |        0.745605 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.781348 | 1.1446  | 0.798924 |        0.733916 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.786828 | 1.14936 | 0.828836 |        0.736536 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.783618 | 1.1502  | 0.821431 |        0.731965 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.782965 | 1.14623 | 0.805527 |        0.734865 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.781714 | 1.153   | 0.867921 |        0.734896 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.782679 | 1.15069 | 0.884223 |        0.734447 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.783261 | 1.15136 | 0.854483 |        0.73472  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.783876 | 1.15158 | 0.920513 |        0.74104  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.785524 | 1.15082 | 0.953638 |        0.742309 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.784415 | 1.15462 | 0.958318 |        0.739383 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.783569 | 1.15283 | 0.963136 |        0.736886 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.784092 | 1.15241 | 0.967744 |        0.737878 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.786108 | 1.15115 | 0.963307 |        0.737638 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.786427 | 1.15185 | 0.968853 |        0.735101 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.786454 | 1.15406 | 0.963981 |        0.734593 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.787199 | 1.15399 | 0.959185 |        0.733013 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.78674  | 1.15389 | 0.954439 |        0.735125 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.787803 | 1.15311 | 0.967336 |        0.731153 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.784684 | 1.15359 | 0.954131 |        0.725449 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.784971 | 1.15589 | 0.95703  |        0.729506 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.784306 | 1.15516 | 0.957145 |        0.726293 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.784903 | 1.15369 | 0.955593 |        0.730319 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.781187 | 1.15656 | 0.955538 |        0.729928 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.781195 | 1.15634 | 0.964412 |        0.730104 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.779162 | 1.15624 | 0.962324 |        0.727798 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.776627 | 1.15861 | 0.962516 |        0.730072 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77702  | 1.16036 | 0.95358  |        0.726916 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775625 | 1.16035 | 0.949514 |        0.729622 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774906 | 1.15922 | 0.959872 |        0.734116 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772811 | 1.16016 | 0.959974 |        0.733506 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.771418 | 1.15959 | 0.958121 |        0.729931 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.769226 | 1.15987 | 0.950862 |        0.730653 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.770437 | 1.15919 | 0.953313 |        0.734905 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772161 | 1.15923 | 0.950526 |        0.727538 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773221 | 1.16019 | 0.958173 |        0.733024 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77383  | 1.16073 | 0.954969 |        0.733589 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773081 | 1.16095 | 0.954809 |        0.729911 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772338 | 1.16127 | 0.953069 |        0.731026 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772906 | 1.16123 | 0.958935 |        0.733184 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772994 | 1.16127 | 0.954553 |        0.726486 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773556 | 1.16175 | 0.939911 |        0.727674 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772818 | 1.16318 | 0.960748 |        0.728218 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77283  | 1.1608  | 0.948698 |        0.730163 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773788 | 1.15958 | 0.950076 |        0.725544 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775468 | 1.15944 | 0.952386 |        0.72964  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774236 | 1.15909 | 0.947311 |        0.72882  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772765 | 1.15934 | 0.9418   |        0.725735 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.771901 | 1.1595  | 0.945876 |        0.723035 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.770343 | 1.16021 | 0.950045 |        0.723597 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.770823 | 1.16078 | 0.942315 |        0.724464 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.770619 | 1.16062 | 0.953339 |        0.728925 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77163  | 1.16069 | 0.952481 |        0.724889 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.771879 | 1.16121 | 0.947957 |        0.729157 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.771718 | 1.16201 | 0.945139 |        0.728651 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77316  | 1.16149 | 0.950052 |        0.725031 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772637 | 1.16177 | 0.948993 |        0.727489 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77314  | 1.16217 | 0.942318 |        0.731939 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774193 | 1.1621  | 0.947238 |        0.726057 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772058 | 1.16367 | 0.944604 |        0.727335 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772559 | 1.16344 | 0.945041 |        0.72817  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772712 | 1.16374 | 0.945667 |        0.732296 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.776383 | 1.16347 | 0.943988 |        0.727633 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.776431 | 1.16422 | 0.945937 |        0.730475 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774009 | 1.16404 | 0.948614 |        0.72733  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774007 | 1.16401 | 0.939614 |        0.726553 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775077 | 1.16353 | 0.946695 |        0.730234 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775618 | 1.16363 | 0.937773 |        0.727532 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775446 | 1.16289 | 0.946614 |        0.728085 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775422 | 1.16302 | 0.937688 |        0.731347 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773934 | 1.16265 | 0.940045 |        0.728231 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773284 | 1.16273 | 0.941666 |        0.726128 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773652 | 1.16275 | 0.940264 |        0.727046 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773487 | 1.16302 | 0.946949 |        0.726193 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773474 | 1.16342 | 0.943642 |        0.72899  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773997 | 1.16364 | 0.944094 |        0.727881 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774181 | 1.16419 | 0.938709 |        0.727225 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774178 | 1.16427 | 0.949845 |        0.723586 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77411  | 1.16416 | 0.944764 |        0.727533 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773588 | 1.16409 | 0.945879 |        0.728441 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772612 | 1.16387 | 0.9419   |        0.726691 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.772233 | 1.16398 | 0.946039 |        0.726755 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.771015 | 1.1648  | 0.94355  |        0.722754 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77132  | 1.16505 | 0.94285  |        0.728158 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773105 | 1.16465 | 0.952159 |        0.724395 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774276 | 1.1642  | 0.945333 |        0.724524 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774548 | 1.16414 | 0.931125 |        0.723169 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774828 | 1.16367 | 0.945349 |        0.72436  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774641 | 1.1637  | 0.942398 |        0.722126 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774675 | 1.16361 | 0.943351 |        0.728619 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774881 | 1.16343 | 0.942501 |        0.727164 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775677 | 1.16227 | 0.951283 |        0.722903 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774495 | 1.1626  | 0.944617 |        0.7269   |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774814 | 1.16232 | 0.945924 |        0.721117 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77398  | 1.16202 | 0.943315 |        0.721965 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77423  | 1.16163 | 0.943851 |        0.725636 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775024 | 1.16152 | 0.939128 |        0.72481  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774761 | 1.16122 | 0.945675 |        0.723079 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.773889 | 1.16092 | 0.951489 |        0.728441 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.774831 | 1.16061 | 0.939066 |        0.71951  |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775304 | 1.16029 | 0.946238 |        0.721934 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.775515 | 1.15993 | 0.949116 |        0.721538 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.776737 | 1.1598  | 0.946615 |        0.723333 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.776828 | 1.15957 | 0.940348 |        0.721449 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.777476 | 1.1592  | 0.93711  |        0.721838 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.779368 | 1.15929 | 0.943326 |        0.724807 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.77927  | 1.15875 | 0.944185 |        0.724135 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.778926 | 1.15852 | 0.944663 |        0.717624 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.780401 | 1.15864 | 0.946098 |        0.723162 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.78097  | 1.15859 | 0.941184 |        0.720327 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.78099  | 1.15864 | 0.964201 |        0.7241   |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.781196 | 1.15855 | 0.948133 |        0.719857 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.787187 | 1.15876 | 0.938202 |        0.720492 |\n",
      "+------------+---------+----------+-----------------+\n",
      "|   0.787795 | 1.15878 | 0.943033 |        0.722308 |\n",
      "+------------+---------+----------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "headers=['Logistic','SVM','MLP','Random Forest']\n",
    "indices=[i for i in range(124)]\n",
    "f=len(indices)\n",
    "labels=list(df['Final Label'])\n",
    "l=len(labels)\n",
    "correct_order=find_order()\n",
    "print(correct_order)\n",
    "confidence,data=correlation_algo(correct_order)\n",
    "cols=[f'feature{k}' for k in indices]\n",
    "X=df[cols].values\n",
    "con,info=ml(X,labels)\n",
    "data.append(info)\n",
    "confidence.append(con)\n",
    "print(\"Accuracy on augmented data\")\n",
    "print(tabulate(data, headers=headers, tablefmt=\"grid\"))\n",
    "print(\"Confidence on augmented data\")\n",
    "print(tabulate(confidence, headers=headers, tablefmt=\"grid\"))\n",
    "file_path1='bert_accuracy.pkl'\n",
    "file_path2='bert_confidence.pkl'\n",
    "with open(file_path1,'wb') as f:\n",
    "    pickle.dump(data,f)\n",
    "with open(file_path2,'wb') as f:\n",
    "    pickle.dump(confidence,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df66afe-b7fb-4db6-be6b-24f9972fe645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
